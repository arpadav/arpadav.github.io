<!doctype html><meta content="text/html; charset=utf-8"charset=utf-8 http-equiv=Content-Type><title>Analysis and Implementation of a Semantic Auto-Encoder for Zero-Short Learning - Arpad Voros</title><link href=/images/site.webmanifest rel=manifest><link href=/images/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/images/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/images/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><script src="https://www.googletagmanager.com/gtag/js?id=G-8RV0KPZZXQ"async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[];gtag(`js`,new Date());gtag(`config`,`G-8RV0KPZZXQ`)</script></head><div class=sidebar id=sidebar><li><a href=/gator/ target=_parent> <img src=/images/alligator.png style=width:268px> </a><li>   <h3><b> personal</b></h3>      <li><a href=/projects/2022f_lang/ target=_parent> <i>Vocabulary-emphasized Language Learning Application using no 'Reference' Language</i> </a><li>§<li><a href=/projects/2020t_website/ target=_parent> <i>Personal Website — arpadav.github.io (hosted on arpadvoros.com)</i> </a><li>§<li><a href=/projects/2019t_plotter/ target=_parent> <i>Physical Plotting Output of Generative Adversarial Network Trained on Landscape Portraits</i> </a><li>§<li><a href=/projects/2018s_mra/ target=_parent> <i>Google Hangouts Chat-bot and iOS 'Alert' Application</i> </a><li>§<li><a href=/projects/2018f_theremin/ target=_parent> <i>Moog Theremin</i> </a><li>§<li><a href=/projects/2017f_mst/ target=_parent> <i>Muon Scattering Tomography: Utilizing Silicon Photomultiplier Arrays to Trilaterate Muon Multiple-Coulomb Scattering Events</i> </a><li>   <h3><b> rust crates</b></h3>      <li><a href=/projects/2024t_tinyklv/ target=_parent> <i>tinyklv: A KLV (Key-Length-Value) parser and generator for Rust</i> </a><li>§<li><a href=/projects/2024t_misb/ target=_parent> <i>misb: MISB Standards implementation in Rust</i> </a><li>§<li><a href=/projects/2024s_thisenum/ target=_parent> <i>thisenum: Assigning constants to enum arms</i> </a><li>§<li><a href=/projects/2024s_dted2/ target=_parent> <i>dted2: DTED reader for Rust</i> </a><li>§<li><a href=/projects/2024f_crosstalk/ target=_parent> <i>crosstalk: In-memory pub-sub messaging for Rust</i> </a><li>   <h3><b> academic</b></h3>      <li><a href=/projects/2021t_zsl/ target=_parent> <i>Analysis and Implementation of a Semantic Auto-Encoder for Zero-Short Learning</i> </a><li>§<li><a href=/projects/2021t_dimg/ target=_parent> <i>Digital Imaging — Scale-invariant Feature Transform, Laplacian Blending, Linear Spatial Filtering, and more...</i> </a><li>§<li><a href=/projects/2021f_vader/ target=_parent> <i>Senior Design — Directional Acoustic Deterrence of Elephants to Prevent Human-Elephant Conflict in sub-Saharan Africa</i> </a><li>§<li><a href=/projects/2021f_cvis/ target=_parent> <i>Computer Vision — Facial Recognition using Gaussian Mixture Models, Adaboost & Haar Features, and more...</i> </a><li>§<li><a href=/projects/2020t_terrain/ target=_parent> <i>Terrain Identification using Sensory Prosthetic Limb Timeseries Data</i> </a><li>§<li><a href=/projects/2020t_crop/ target=_parent> <i>Semantic Segmentation of Crop Damage</i> </a><li>§<li><a href=/projects/2020f_hybrid/ target=_parent> <i>Adjusting Hybrid-Energy Model of Photovoltaic Generators to fit Klucher Weather Model</i> </a><li>§<li><a href=/projects/2018s_nedm/ target=_parent> <i>Undergraduate Research for Intercollegiate Search for the Neutron Electric Dipole-Moment</i> </a><li>   <h3><b> mini projects</b></h3>      <li><a href=/projects/2023t_cgol/ target=_parent> <i>Conway's Game of Life using Rust + WASM</i> </a><li>§<li><a href=/projects/2017t_gdax/ target=_parent> <i>High School - Cryptocurrency Trading Bot</i> </a></div><div class=beside-sidebar><div class=navbar id=navbar><ul><li><a href=/# target=_parent>home</a><li><a href=/#about target=_parent>about</a><li><a href=/#contact target=_parent>contact</a><li><a href=/#socials target=_parent>socials</a><li><a href=/cv.pdf target=_parent>cv</a><li><a href=/projects target=_parent>projects</a><li><a href=/notes target=_parent>notes</a></ul></div><div class=project-body><h3>Analysis and Implementation of a Semantic Auto-Encoder for Zero-Short Learning</h3><i><p>  Independent study in few-short learning architecture</i><hr><h4>  Individual Project — <i><span style=font-weight:400>Status:</span> Complete</i></h4><p>  <b>Aug 2021 - Dec 2021</b>    — <i>Raleigh, North Carolina</i><p>Repositories</p>    — <a href=https://github.com/arpadav/zsl_sae_matlab>zsl_sae_matlab</a><hr><p><i>Overview:</i><p>Zero-shot learning (or ZSL) is a field of study in deep learning to use a preexisting model, without altering it, to perform a task (such as classification) it has never performed before. An example with classification would be to classify a set of classes it has not been trained on. Using a semantic auto-encoder, the final feature layers of some classifier can be projected into some meaningful latent space of varying dimensionality. This semantic space is then used as the basis of classification, rather than using the feature-space or label / classification space. Depending on how classification is performed, this semantic auto-encoder solution for the ZSL paradigm is suprisingly effective. An implementation and analysis of this (work of <a href=https://arxiv.org/abs/1704.08345>Kodirova, Xiang, and Gong in 2017</a>) is made, as seen in the report at the bottom of this page. More detail about ZSL and this study is all fully recorded in the report.<p>In addition, a decoder is used (independent of the SAE for ZSL) for purely cosmetic purposes to visualize the spaces being worked in, and gain a more intuitive understanding of how this SAE works how it does. The figure below shows both the classifier, the intermediate SAE for ZSL, as well as the futile decoder.</p><img class=center src=/projects/2021t_zsl/arch.png style=max-width:400px><p class=center_desc>Fig 1: <i>Model architecture used in this study</i><hr><p><i>Personal Statement:</i><p>I decided to take ECE 633 (Individual Topics in Electrical Engineering, or commonly refered to as an independent study) at NC State University so I could have a masters project, similar to a thesis, while completing my class-based masters. Though this project is significantly smaller than a thesis, it still gave me a 'taste' of what a thesis-based masters would be like.<p>The original idea I had has been in my mind for quite sometime, and in-fact, I still have yet to implement it. The idea has to do with initial impressions, and how, as humans, we greatly take in / consider / account for information when it is a <i>first impression</i>. Therefore, I proposed a simple neural network architecture which dynamically can append nodes to the final classification layer, based off whether a point of training data has never been seen before during training. This would not be trained in the traditional sense, but rather sequentially trained. Therefore, initial training is very extensive and slow, but exponentially decreases the more classes are added; the model learns at the rate which it knows. E.g., if hand-drawn numeric characters are attempted to being learned by the model for classification, then first only 0's are trained, then mixes of 0's and 1's, then mixes of 0, 1, 2's, etc. until all classes are learned (where each initial impression appends a new classification node, previous weights + biases, and dynamically reconfigures itself accordingly per new class). There are many ways to accomplish this dynamic updating, but the simpliest way is to consider the linear combination of the previous layer to existing nodes, and realise that this linear combination refers to a new node. For the first instance, a new layer (or multiple layers) with no bias and unit weights can be implemented to priotize that linear combination to a select node while attempting to unalter the remaining classifications. Or this step (the appending of new layers) can be completely bypassed by exponentially increasing the classification rate to the new node so that after softmax, it is prefered over the rest. The problem with this, would be a greater misclassification rate on already learned nodes. This also begs the question, on a dynamic classification model where the classification nodes are not held in the same layer. Regardless, this dynamically changing network could also be implemented with unsupervised learning, which was my original intention for the independent study. Where instead of knowing what classes are within a training set, there can be a set number of classification nodes to correspond to features within an image. When a new, unseen feature is introduced to the model, it can determine a confidence level of whether it thinks it has seen this feature before. If not, append a new node. If so, then continue training. This can selectively choose what features are deemed most important, similar to how a CNN works. Then, this can be used as a 'base' model to extract further, class-based information on the dataset trained which will <i>significantly</i> speed up the training process. E.g., this unsupervised model can be trained on faces. The model will select features, becoming some base 'feature model' for the face dataset. Then, if someone wants a model to read emotions, this feature model can act as a base with only 1-2 additional layers added for classification where gradient descent is only done on those 1-2 layers (the feature model remains unaltered). Therefore, instead of making a whole new face model just for emotion extraction, you just use the face feature model and append to it.<p>I still have to attempt this, however my advisor Dr. Tianfu (Matt) Wu dissuaded me for two reasons. One, for a single semester independent study, this is a lot. And two, I am working in the feature space instead of the latent space, which has severe limitations in terms of model flexibility.<p>In addition, I really wish I spent more time on this project. In all, this report is less than a couple weeks worth of work severly crammed in at the end of my degree. Since I completed my masters education in 1 semester after undegrad, every semester I overloaded the amount of classes and work I had to to. The semester this project was done is no exception. If I could put my full attention into this project, significantly better results and (I believe) potentially some novelty could have been achieved.<hr><p><i>Literature:</i></p><a href=/notes/academic/2021%20-%20ECE%20633/Voros_Arpad_ECE633_study.pdf>Voros_Arpad_ECE633_study.pdf</a><br><br><iframe class=center height=950 src=/notes/academic/2021%20-%20ECE%20633/Voros_Arpad_ECE633_study.pdf width=60%></iframe><br></div><base target=_top></div><link href=/css/std.css rel=stylesheet>